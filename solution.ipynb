{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092fc3d9",
   "metadata": {},
   "source": [
    "# DA5401 A8: Ensemble Learning on Bike Sharing\n",
    "## Introduction\n",
    "\n",
    "This notebook builds and compares ensemble regressors to forecast hourly bike rentals (cnt) using the UCI Bike Sharing dataset. We load hour.csv (with day.csv for context), prepare features (drop instant, dteday, casual, registered; encode time and weather categories), and evaluate models using RMSE on a held-out test set.\n",
    "\n",
    "Goals\n",
    "- Establish simple baselines: Decision Tree (max_depth=6) and Linear Regression.\n",
    "- Reduce variance with Bagging (trees, n_estimators ≥ 50).\n",
    "- Reduce bias with Gradient Boosting.\n",
    "- Combine diverse learners via Stacking (KNN, Bagging, Gradient Boosting) with a Ridge meta-learner.\n",
    "\n",
    "Deliverables\n",
    "- Clean, reproducible code and brief plots.\n",
    "- A results table comparing RMSE across all models.\n",
    "- A short conclusion explaining which model performs best and why, referencing bias–variance and model diversity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45434a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a0623b",
   "metadata": {},
   "source": [
    "We begin by loading the data and dropping irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d56d3f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (17379, 17)\n",
      "Features kept: ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
      "Categorical: ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
      "Numeric: ['temp', 'atemp', 'hum', 'windspeed']\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and basic feature setup\n",
    "\n",
    "# 1) Load data\n",
    "df = pd.read_csv(\"hour.csv\")\n",
    "\n",
    "# 2) Target\n",
    "y = df[\"cnt\"].copy()\n",
    "\n",
    "# 3) Drop irrelevant/leaky columns\n",
    "drop_cols = [\"instant\", \"dteday\", \"casual\", \"registered\"]\n",
    "X = df.drop(columns=drop_cols + [\"cnt\"])\n",
    "\n",
    "# 4) Define feature groups for preprocessing\n",
    "categorical_cols = [\"season\", \"yr\", \"mnth\", \"hr\", \"holiday\", \"weekday\", \"workingday\", \"weathersit\"]\n",
    "numeric_cols = [\"temp\", \"atemp\", \"hum\", \"windspeed\"]\n",
    "\n",
    "# Quick sanity checks\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Features kept:\", X.columns.tolist())\n",
    "print(\"Categorical:\", categorical_cols)\n",
    "print(\"Numeric:\", numeric_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb540765",
   "metadata": {},
   "source": [
    "Now, let's construct  clean, reusable preprocessing step that one-hot encodes the categorical columns and leaves numeric columns untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c4654",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Categorical and numeric columns defined earlier\n",
    "categorical_cols = [\"season\", \"yr\", \"mnth\", \"hr\", \"holiday\", \"weekday\", \"workingday\", \"weathersit\"]\n",
    "numeric_cols = [\"temp\", \"atemp\", \"hum\", \"windspeed\"]\n",
    "\n",
    "# One-Hot Encoder with safe handling of unseen categories\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "# Column-wise transformer: OHE for categoricals, passthrough for numerics\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", ohe, categorical_cols),\n",
    "        (\"num\", \"passthrough\", numeric_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba2ba5f",
   "metadata": {},
   "source": [
    "We then use a chronological split to respect time order and avoid leakage from the future into the past. This is the correct choice for hourly demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf1343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chronological 80/20 split\n",
    "n = len(X)\n",
    "cut = int(0.8 * n)\n",
    "\n",
    "X_train, X_test = X.iloc[:cut], X.iloc[cut:]\n",
    "y_train, y_test = y.iloc[:cut], y.iloc[cut:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f49c8cc",
   "metadata": {},
   "source": [
    "Next we proceed to fitting a bseline regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13c90e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (depth=6, min_leaf=10) RMSE: 158.723\n",
      "Linear Regression RMSE: 133.347\n",
      "Baseline: Linear Regression | RMSE = 133.347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1) Encode train/test explicitly (no pipelines)\n",
    "X_train_enc = preprocess.fit_transform(X_train)   # preprocess is the ColumnTransformer defined earlier\n",
    "X_test_enc  = preprocess.transform(X_test)\n",
    "\n",
    "# 2) Decision Tree (depth=6) with light regularization\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "dt = DecisionTreeRegressor(\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=10,    # small regularization for stability\n",
    "    random_state=42\n",
    ")\n",
    "dt.fit(X_train_enc, y_train)\n",
    "y_pred_dt = dt.predict(X_test_enc)\n",
    "rmse_dt = mean_squared_error(y_test, y_pred_dt, squared=False)\n",
    "print(f\"Decision Tree (depth=6, min_leaf=10) RMSE: {rmse_dt:.3f}\")\n",
    "\n",
    "# 3) Linear Regression (no normalization; keep intercept)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression(fit_intercept=True)\n",
    "lr.fit(X_train_enc, y_train)\n",
    "y_pred_lr = lr.predict(X_test_enc)\n",
    "rmse_lr = mean_squared_error(y_test, y_pred_lr, squared=False)\n",
    "print(f\"Linear Regression RMSE: {rmse_lr:.3f}\")\n",
    "\n",
    "# 4) Choose baseline\n",
    "baseline_name = \"Decision Tree (depth=6)\" if rmse_dt <= rmse_lr else \"Linear Regression\"\n",
    "baseline_rmse = min(rmse_dt, rmse_lr)\n",
    "print(f\"Baseline: {baseline_name} | RMSE = {baseline_rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83beaf6b",
   "metadata": {},
   "source": [
    "Linear Regression RMSE 133.347 vs Decision Tree (depth=6, min_leaf=10) RMSE 158.723 indicates the linear model generalizes better on your held-out chronological test set. That suggests the shallow tree, even with light regularization, is underfitting important interactions or overfitting to local splits that don’t transfer well across time, while the linear model benefits from one-hot encoded time/weather signals that are fairly additive over the horizon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f8ffc0",
   "metadata": {},
   "source": [
    "Below is a bagging implementation using  Decision Tree (depth=6) as the base estimator, with 100 estimators and standard bagging settings, followed by RMSE evaluation and a short discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866b871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging (100x DT depth=6, min_leaf=10) RMSE: 155.736\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Base learner: match the baseline tree spec\n",
    "base_tree = DecisionTreeRegressor(\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Bagging: variance reduction via bootstrap aggregation\n",
    "bag = BaggingRegressor(\n",
    "    estimator=base_tree,\n",
    "    n_estimators=100,       # >= 50 as required\n",
    "    max_samples=1.0,        # bootstrap over samples\n",
    "    max_features=1.0,       # use all features per base estimator\n",
    "    bootstrap=True,         # sample rows with replacement\n",
    "    bootstrap_features=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit on encoded training data, evaluate on encoded test data\n",
    "bag.fit(X_train_enc, y_train)\n",
    "y_pred_bag = bag.predict(X_test_enc)\n",
    "rmse_bag = root_mean_squared_error(y_test, y_pred_bag)\n",
    "print(f\"Bagging (100x DT depth=6, min_leaf=10) RMSE: {rmse_bag:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a908e04d",
   "metadata": {},
   "source": [
    "Bagging improved over the single Decision Tree (158.723 → 155.736 RMSE), but the gain is modest, indicating some variance reduction from averaging bootstrap samples of the same shallow base learner.\n",
    "\n",
    "The limited improvement is consistent with bias constraints: a depth-6 tree with min_samples_leaf=10 is already strongly regularized, so bagging can only reduce variance on top of a relatively high-bias model.\n",
    "\n",
    "In practice, bagging tends to help more when base trees are higher-variance (e.g., deeper or less constrained), whereas with shallow trees the ensemble mainly smooths noise without addressing structural underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872330d9",
   "metadata": {},
   "source": [
    "We now, proceed with gradient boosting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16f37aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting RMSE: 110.447\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=300,     # enough stages to reduce bias\n",
    "    learning_rate=0.05,  # smaller rate + more estimators = smoother fit\n",
    "    max_depth=3,         # shallow trees as weak learners (common default)\n",
    "    subsample=1.0,       # pure boosting (set <1.0 for stochastic GB)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gbr.fit(X_train_enc, y_train)\n",
    "y_pred_gbr = gbr.predict(X_test_enc)\n",
    "rmse_gbr = root_mean_squared_error(y_test, y_pred_gbr)\n",
    "print(f\"Gradient Boosting RMSE: {rmse_gbr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6133d",
   "metadata": {},
   "source": [
    "Gradient Boosting clearly outperforms both baselines on  chronological test split: 110.447 vs Linear Regression 133.347 and Bagging 155.736, indicating substantial error reduction.\n",
    "\n",
    "This aligns with the hypothesis that boosting primarily reduces bias: by sequentially fitting shallow trees to residuals, the model captures nonlinear interactions between time and weather effects that linear regression misses and that bagging with shallow trees cannot correct.\n",
    "\n",
    "The gap from 133.347 to 110.447 is sizable, suggesting meaningful structure beyond additive linear terms; the improvement over bagging also shows that averaging similar shallow trees wasn’t enough, whereas boosting’s stage-wise corrections addressed systematic underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144a9270",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "Stacking (stacked generalization) is a two-layer ensemble where a meta-learner is trained to combine the predictions of several diverse base learners into a single, stronger prediction.\n",
    "\n",
    "How it works\n",
    "- Level-0 (base learners): Train multiple different models on the same training data (e.g., KNN, bagging trees, gradient boosting). Each model produces its own prediction for every sample. The key is diversity—models should make different kinds of errors so their predictions carry complementary information.\n",
    "- Out-of-fold predictions: To train the meta-learner without leaking target information, generate base-learner predictions on held-out folds of the training set (out-of-fold). Concatenate these predictions into a new feature matrix where each column is a base model’s prediction; the target remains the original y. This ensures the meta-learner sees predictions for each training sample that were made without training on that sample.\n",
    "- Level-1 (meta-learner): Fit a simple, regularized model (e.g., Ridge regression) on the out-of-fold prediction matrix to learn how to weight and combine base predictions. At test time, get base-model predictions on the test set, stack them into the same column order, and feed them to the meta-learner for the final prediction.\n",
    "- Why it helps: The meta-learner learns patterns of agreement/disagreement among base models and assigns higher weight to models that are more reliable in specific regions of the feature space, effectively reducing both bias (by blending nonlinear learners) and variance (by averaging across diverse errors). Unlike bagging (averaging similar models) or boosting (sequentially correcting residuals), stacking learns an explicit mapping from base predictions to the target, which can capture when to trust which model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933e715",
   "metadata": {},
   "source": [
    "We'll use the following as Level-0 (base) learners:\n",
    "\n",
    "K-Nearest Neighbors Regressor (KNeighborsRegressor)\n",
    "\n",
    "Bagging Regressor (DecisionTreeRegressor base, from earlier)\n",
    "\n",
    "Gradient Boosting Regressor (from earlier)\n",
    "\n",
    "Our Level-1 (meta-learner) will be Ridge Regression, which can learn optimal weights for each base learner’s predictions and helps avoid overfitting through regularization.\n",
    "\n",
    "Implementing StackingRegressor in scikit-learn\n",
    "StackingRegressor automatically fits the base estimators on training data, generates out-of-fold predictions to train the meta-learner (Ridge), and predicts on new data by combining base predictions according to the learned coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a4f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Regressor RMSE: 103.803\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define base learners (Level-0)\n",
    "knn = KNeighborsRegressor(n_neighbors=10)  \n",
    "bag = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(max_depth=6, min_samples_leaf=10, random_state=42),\n",
    "    n_estimators=100,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    subsample=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Meta-learner (Level-1)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "\n",
    "# Combine all in a StackingRegressor\n",
    "stack = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('knn', knn),\n",
    "        ('bag', bag),\n",
    "        ('gbr', gbr)\n",
    "    ],\n",
    "    final_estimator=ridge,\n",
    "    cv=5,             # 5-fold cross validation for out-of-fold base predictions\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on training data and predict/test\n",
    "stack.fit(X_train_enc, y_train)\n",
    "y_pred_stack = stack.predict(X_test_enc)\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "rmse_stack = root_mean_squared_error(y_test, y_pred_stack)\n",
    "print(f\"Stacking Regressor RMSE: {rmse_stack:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5ea134",
   "metadata": {},
   "source": [
    "# Stacking Regressor Performance and Interpretation\n",
    "\n",
    "**Stacking Regressor RMSE:** 103.803\n",
    "\n",
    "The stacking ensemble achieved the lowest RMSE among all your tested models (linear regression, bagging, and gradient boosting). This result strongly suggests that stacking has leveraged the strengths of different base learners—KNN, Bagging, and Gradient Boosting—while the Ridge regression meta-learner found an optimal combination for their outputs. A lower RMSE indicates your model’s predictions are, on average, closer to the true bike rental counts than the alternatives, in line with best practices for evaluating regression models.\n",
    "\n",
    "**Why does stacking help?**\n",
    "- **Model diversity:** Each base model learns different aspects and patterns in the data; KNN captures locality, bagging reduces variance, and boosting reduces bias.\n",
    "- **Bias-variance tradeoff:** By combining diverse models, stacking can reduce both bias and variance more effectively than bagging or boosting alone.\n",
    "- **Optimal blending:** The Ridge meta-learner adapts to favor base models where they perform best, learning to weight predictions for maximal accuracy on held-out (out-of-fold) data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd700979",
   "metadata": {},
   "source": [
    "## RMSE Comparison Table\n",
    "\n",
    "Below is a table summarizing the Root Mean Squared Error (RMSE) on the test set for each ensemble and baseline model:\n",
    "\n",
    "| Model                        | Test RMSE  |\n",
    "|------------------------------|------------|\n",
    "| **Linear Regression (Baseline)**  | 133.347    |\n",
    "| **Decision Tree Regressor**       | 158.723    |\n",
    "| **Bagging Regressor**             | 155.736    |\n",
    "| **Gradient Boosting Regressor**   | 110.447    |\n",
    "| **Stacking Regressor**            | 103.803    |\n",
    "\n",
    "**Interpretation:**  \n",
    "- The stacking ensemble achieved the lowest RMSE, indicating the most accurate predictions on the test data.\n",
    "- Gradient boosting and stacking both substantially outperformed the single-model baselines, confirming the power of bias reduction and diverse model combination.\n",
    "- Bagging (with shallow trees) reduced variance only slightly compared to the single tree, which is expected given high bias in depth-limited models.\n",
    "\n",
    "> *RMSE values are rounded to three decimal places for clarity. Lower RMSE indicates better predictive accuracy for this regression task.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016a8cd5",
   "metadata": {},
   "source": [
    "## Conclusion: Best Model & Why Stacking Wins\n",
    "\n",
    "**Best-performing model:** The Stacking Regressor achieved the lowest test RMSE (103.803), outperforming the baseline Linear Regression (133.347), Decision Tree (158.723), Bagging (155.736), and Gradient Boosting (110.447).\n",
    "\n",
    "**Why Stacking Regressor outperformed single models:**\n",
    "\n",
    "- **Model Diversity:** Stacking combines very different types of base learners: K-Nearest Neighbors (captures local patterns), Bagging of Decision Trees (reduces variance), and Gradient Boosting (reduces bias). Since each of these models has unique strengths and weaknesses, their individual prediction errors are less correlated. This diversity allows the meta-learner (Ridge Regression) to learn how to weight and blend the base models' predictions to minimize errors in a way that any one model alone cannot.\n",
    "\n",
    "- **Bias-Variance Trade-off:** Single models like linear regression are low-variance but high-bias and can't capture nonlinearities. A single decision tree can capture some complexity but may have high variance (especially if deep). Bagging shrinks variance by averaging many trees, but doesn’t reduce their bias. Boosting reduces bias through sequential error correction. Stacking moves beyond both: the meta-learner learns the optimal combination, lowering both bias and variance if the base learners are sufficiently different.\n",
    "\n",
    "- **Optimal Blending:** The Ridge meta-learner is trained on out-of-fold predictions from each base model, preventing overfitting and ensuring the meta-learner adapts to situations where one model is more accurate than others. This data-driven combination leads to more robust, accurate predictions than picking any single approach.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
